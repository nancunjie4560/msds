install.packages("AppliedPredictiveModeling")
install.packages("ChemicalManufacturingProcess")
knitr::opts_chunk$set(echo = TRUE)
#Load the data
data(ChemicalManufacturingProcess)
df = data.frame(ChemicalManufacturingProcess)
#Load the data
data(chemicalManufacturingProcess)
#Load the data
library(AppliedPredictiveModeling)
data(chemicalManufacturingProcess)
data(ChemicalManufacturingProcess)
df = data.frame(ChemicalManufacturingProcess)
#Filter to only Yield and sort in descending order
yield_corr = filter(melted_cormat_df, Var2 == "Yield")
#Fill in missing values with the median of each feature
for(i in 1:ncol(df)){
df[is.na(df[,i]), i] <- median(df[,i], na.rm = TRUE)}
#Filter to only Yield and sort in descending order
yield_corr = filter(melted_cormat_df, Var2 == "Yield")
#Create the regression model
library(AppliedPredictiveModeling)
install.packages("caret")
library(caret)
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knitr::opts_chunk$set(echo = TRUE, eval=F)
#Load the data
data(ChemicalManufacturingProcess)
library(AppliedPredictiveModeling)
#Load the data
data(ChemicalManufacturingProcess)
df = data.frame(ChemicalManufacturingProcess)
head(df)
#Fill in missing values with the median of each feature
for(i in 1:ncol(df)){
df[is.na(df[,i]), i] <- median(df[,i], na.rm = TRUE)}
library(AppliedPredictiveModeling)
#Load the data
data(ChemicalManufacturingProcess)
df = data.frame(ChemicalManufacturingProcess)
head(df)
library(AppliedPredictiveModeling)
#Load the data
data(ChemicalManufacturingProcess)
df = data.frame(ChemicalManufacturingProcess)
head(df)
#predict with the test data
predict(lmyield, newdata = test, interval ='prediction')
library(mlbench)
library(caret)
library(AppliedPredictiveModeling)
library(kernlab)
install.packages("kernlab")
library(kernlab)
library(mlbench)
library(caret)
library(AppliedPredictiveModeling)
library(kernlab)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the ' x ' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
#featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector ' y ' and a matrix
## of predictors ' x ' . Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
svmModel <- train(x = trainingData$x,
y = trainingData$y,
method = "svmRadial",
tuneLength=10,
preProc = c("center", "scale"))
svmModel
svmPred <- predict(svmModel, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
nnetGrid <- expand.grid(.decay=c(0, 0.01, 0.1, 0.5, 0.9),
.size=c(1, 10, 15, 20),
.bag=FALSE)
nnetModel <- train(x = trainingData$x,
y = trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
trace=FALSE,
linout=TRUE,
maxit=500)
library(doparallel)
install.packages("doParallel")
library(doParallel)
library(mlbench)
library(caret)
library(AppliedPredictiveModeling)
library(kernlab)
library(doParallel)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the ' x ' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
#featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector ' y ' and a matrix
## of predictors ' x ' . Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
svmModel <- train(x = trainingData$x,
y = trainingData$y,
method = "svmRadial",
tuneLength=10,
preProc = c("center", "scale"))
svmModel
svmPred <- predict(svmModel, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
nnetGrid <- expand.grid(.decay=c(0, 0.01, 0.1, 0.5, 0.9),
.size=c(1, 10, 15, 20),
.bag=FALSE)
nnetModel <- train(x = trainingData$x,
y = trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
trace=FALSE,
linout=TRUE,
maxit=500)
marsModel <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
library(earth)
marsModel <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
marsGrid <- expand.grid(.degree=1:2,
.nprune=2:20)
marsModel <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
postResample(pred = marsPred, obs = testData$y)
marsPred <- predict(marsModel, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)
varImp(marsModel)
system.time(foreach(i=1:10000) %do% sum(tanh(1:i)))
system.time(foreach(i=1:10000) %dopar% sum(tanh(1:i)))
registerDoParallel()
getDoParWorkers()
registerDoSEQ()
getDoParWorkers()
registerDoParallel(cores=2)
getDoParWorkers()
library(doParallel) # Used for computation
registerDoParallel(cores=2)
getDoParWorkers()
library(mlbench)
library(caret)
library(AppliedPredictiveModeling)
library(kernlab)
library(doParallel) # Used for computation
library(earth) # Package necessary for marsModel
registerDoParallel(cores=2)
getDoParWorkers()
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the ' x ' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
#featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector ' y ' and a matrix
## of predictors ' x ' . Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
knnModel
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
svmModel <- train(x = trainingData$x,
y = trainingData$y,
method = "svmRadial",
tuneLength=10,
preProc = c("center", "scale"))
svmModel
svmPred <- predict(svmModel, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
nnetGrid <- expand.grid(.decay=c(0, 0.01, 0.1, 0.5, 0.9),
.size=c(1, 10, 15, 20),
.bag=FALSE)
nnetModel <- train(x = trainingData$x,
y = trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
trace=FALSE,
linout=TRUE,
maxit=500)
# ###Warning: executing %dopar% sequentially: no parallel backend registered###
nnetPred <- predict(nnetModel, newdata = testData$x)
postResample(pred = nnetPred, obs = testData$y)
marsGrid <- expand.grid(.degree=1:2,
.nprune=2:20)
marsModel <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
marsPred <- predict(marsModel, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)
varImp(marsModel)
library(mlbench)
library(randomForest)
install.packages("randomForest")
library(randomForest)
library(caret)
install.packages("partykit")
library(partykit)
library(dplyr)
install.packages("gbm")
install.packages("Cubist")
library(gbm)
library(Cubist)
library(mlbench)
library(randomForest)
library(caret)
library(partykit)
library(dplyr)
library(gbm)
library(Cubist)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
model1 <- randomForest(y ~ .,
data = simulated,
importance = TRUE,
ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
rfImp1
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
model2 <- randomForest(y ~ .,
data = simulated,
importance = TRUE,
ntree = 1000)
rfImp2 <- varImp(model2, scale = FALSE)
rfImp2
cforest_model <- cforest(y ~ ., data=simulated)
# Unconditional importance measure
varimp(cforest_model) %>% sort(decreasing = T)
varimp(cforest_model, conditional=T) %>% sort(decreasing = T)
gbm_Model <- gbm(y ~ ., data=simulated, distribution='gaussian')
summary(gbm_Model)
cubistModel <- cubist(x=simulated[,-(ncol(simulated)-1)], y=simulated$y, committees=100)
varImp(cubistModel)
library(caret)
library(rpart)
set.seed(755)
X1 <- rep(1:2, each=100)
Y <- X1 + rnorm(600, mean=2, sd=4)
X2 <- rnorm(600, mean=2, sd=4)
simData <- data.frame(Y=Y, X1=X1, X2=X2)
fit <- rpart(Y ~ ., data = simData)
varImp(fit)
library(gbm)
library(AppliedPredictiveModeling)
data(solubility)
grid1 <- expand.grid(n.trees=100, interaction.depth=1, shrinkage=0.1, n.minobsinnode=10)
gbm1 <- train(x = solTrainXtrans, y = solTrainY, method = 'gbm', tuneGrid = grid1, verbose = FALSE)
grid2 <- expand.grid(n.trees=100, interaction.depth=10, shrinkage=0.1, n.minobsinnode=10)
gbm2 <- train(x = solTrainXtrans, y = solTrainY, method = 'gbm', tuneGrid = grid2, verbose = FALSE)
varImp(gbm1)
varImp(gbm2)
df <- read.csv(
"https://raw.githubusercontent.com/palmorezm/msds/main/624/Data/hw_2_data/GroceryDataSet.csv")
library(arules)
library(arulesViz)
library(RColorBrewer)
library(visNetwork)
library(igraph)
df <- read.csv(
"https://raw.githubusercontent.com/palmorezm/msds/main/624/Data/hw_2_data/GroceryDataSet.csv")
df_sparse <- read.transactions("GroceryDataset.csv",format="basket",sep=",")
library(arules)
library(arulesViz)
library(RColorBrewer)
library(visNetwork)
library(igraph)
df <- read.csv(
"https://raw.githubusercontent.com/palmorezm/msds/main/624/Data/hw_2_data/GroceryDataSet.csv")
df_sparse <- read.transactions(
"https://raw.githubusercontent.com/palmorezm/msds/main/624/Data/hw_2_data/GroceryDataSet.csv",
format="basket",sep=",")
summary(df_sparse)
itemFrequencyPlot(df_sparse,topN=20,type="absolute",col=brewer.pal(8,'Pastel2'), main="Frequently Purchased Products")
association.rules <- apriori(df_sparse, parameter = list(supp=0.004, conf=0.3))
length(association.rules)
inspect(sort(association.rules, by = 'lift')[1:10])
subset.rules <- which(colSums(is.subset(association.rules, association.rules)) > 1) # get subset rules in vector
length(subset.rules)
subset.association.rules. <- association.rules[-subset.rules] # remove subset rules.
inspect(sort(subset.association.rules., by = 'lift')[1:10])
plot(subset.association.rules.,method="two-key plot")
plot(association.rules)
top10subRules <- head(subset.association.rules., n = 10, by = "lift")
plot(top10subRules, method = "graph",  engine = "htmlwidget")
subRules2<-head(subset.association.rules., n=10, by="lift")
plot(subRules2, method="paracoord")
